{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5240969-a237-43ec-b6eb-2fc97a48acdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in d:\\anaconda\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in d:\\anaconda\\lib\\site-packages (0.23.0)\n",
      "Requirement already satisfied: torchaudio in d:\\anaconda\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\anaconda\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\anaconda\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\anaconda\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in d:\\anaconda\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: absl-py in d:\\anaconda\\lib\\site-packages (from mediapipe) (2.3.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in d:\\anaconda\\lib\\site-packages (from mediapipe) (25.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in d:\\anaconda\\lib\\site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in d:\\anaconda\\lib\\site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: sentencepiece in d:\\anaconda\\lib\\site-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in d:\\anaconda\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=3 in d:\\anaconda\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in d:\\anaconda\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in d:\\anaconda\\lib\\site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in d:\\anaconda\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install mediapipe opencv-python numpy matplotlib scikit-learn tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e32e2aa4-fcf1-4125-9cef-79ca69ff2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48dbf509-3e12-45d5-95c6-7625742de5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filtered samples: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "dataset_root = r\"D:\\Data science\\datasets\\Sign language Videos\"\n",
    "videos_folder = os.path.join(dataset_root, \"videos\")\n",
    "json_file = os.path.join(dataset_root, \"WLASL_v0.3.json\")\n",
    "\n",
    "with open(json_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Filter only 20 words and map to local video path\n",
    "WORDS = [\n",
    "    \"book\", \"computer\", \"chair\", \"clothes\", \"candy\",\n",
    "    \"drink\", \"go\", \"walk\", \"help\", \"eat\",\n",
    "    \"deaf\", \"fine\", \"thin\", \"black\", \"big\",\n",
    "    \"who\", \"no\", \"yes\", \"before\", \"all\"\n",
    "]\n",
    "\n",
    "dataset = []\n",
    "for entry in data:\n",
    "    if entry[\"gloss\"] in WORDS:\n",
    "        for instance in entry[\"instances\"]:\n",
    "            # assuming video files are named by instance_id\n",
    "            video_filename = f\"{instance['instance_id']}.mp4\"  # adjust if file extension differs\n",
    "            video_path = os.path.join(videos_folder, video_filename)\n",
    "            if os.path.exists(video_path):\n",
    "                dataset.append({\n",
    "                    \"word\": entry[\"gloss\"],\n",
    "                    \"video_path\": video_path\n",
    "                })\n",
    "\n",
    "print(\"Total filtered samples:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6fb717-8d77-4800-844c-b37c33e52dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00335.mp4', '00336.mp4', '00338.mp4', '00339.mp4', '00341.mp4', '00376.mp4', '00377.mp4', '00381.mp4', '00382.mp4', '00384.mp4', '00414.mp4', '00415.mp4', '00416.mp4', '00421.mp4', '00426.mp4', '00430.mp4', '00431.mp4', '00433.mp4', '00435.mp4', '00583.mp4']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "videos_folder = r\"D:\\Data science\\datasets\\Sign language Videos\\videos\"\n",
    "video_files = os.listdir(videos_folder)\n",
    "print(video_files[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "233fc716-68d8-48c7-ac85-cde376f28ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in JSON: 2000\n",
      "First entry keys: dict_keys(['gloss', 'instances'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_file = r\"D:\\Data science\\datasets\\Sign language Videos\\WLASL_v0.3.json\"\n",
    "\n",
    "with open(json_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Total words in JSON:\", len(data))\n",
    "print(\"First entry keys:\", data[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6a3e678-6817-4514-85e9-80e3a5ae0c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filtered samples: 223\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load JSON data\n",
    "json_file = r\"D:\\Data science\\datasets\\Sign language Videos\\WLASL_v0.3.json\"\n",
    "with open(json_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the list of gloss labels you're interested in\n",
    "target_glosses = [\"book\", \"computer\", \"chair\", \"clothes\", \"candy\", \"drink\", \"go\", \"walk\", \"help\", \"eat\", \"deaf\", \"fine\", \"thin\", \"black\", \"big\", \"who\", \"no\", \"yes\", \"before\", \"all\"]\n",
    "\n",
    "# Initialize a list to store the filtered video paths\n",
    "filtered_videos = []\n",
    "\n",
    "# Iterate through the JSON data and match with local files\n",
    "for entry in data:\n",
    "    gloss = entry[\"gloss\"]\n",
    "    if gloss in target_glosses:\n",
    "        for instance in entry[\"instances\"]:\n",
    "            video_id = instance[\"video_id\"]\n",
    "            video_filename = f\"{video_id}.mp4\"  # Assuming the video files are named using video_id\n",
    "            video_path = os.path.join(videos_folder, video_filename)\n",
    "            if os.path.exists(video_path):\n",
    "                filtered_videos.append({\n",
    "                    \"gloss\": gloss,\n",
    "                    \"video_path\": video_path,\n",
    "                    \"split\": instance[\"split\"]\n",
    "                })\n",
    "\n",
    "print(f\"Total filtered samples: {len(filtered_videos)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7290a514-1e23-4e09-beac-aabecb805468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: all\n",
      "1: before\n",
      "2: big\n",
      "3: black\n",
      "4: book\n",
      "5: candy\n",
      "6: chair\n",
      "7: clothes\n",
      "8: computer\n",
      "9: deaf\n",
      "10: drink\n",
      "11: eat\n",
      "12: fine\n",
      "13: go\n",
      "14: help\n",
      "15: no\n",
      "16: thin\n",
      "17: walk\n",
      "18: who\n",
      "19: yes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labels = [item[\"gloss\"] for item in filtered_videos]\n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "encoded_labels = le.transform(labels)\n",
    "\n",
    "for i, word in enumerate(le.classes_):\n",
    "    print(f\"{i}: {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fd3b0c4-5136-45f9-8a55-a9b3a9e5dc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book: 6\n",
      "drink: 15\n",
      "computer: 14\n",
      "before: 16\n",
      "chair: 7\n",
      "go: 15\n",
      "clothes: 5\n",
      "who: 14\n",
      "candy: 13\n",
      "deaf: 11\n",
      "fine: 9\n",
      "help: 14\n",
      "no: 11\n",
      "thin: 16\n",
      "walk: 11\n",
      "yes: 12\n",
      "all: 8\n",
      "black: 10\n",
      "eat: 7\n",
      "big: 9\n",
      "\n",
      "Sorted by word:\n",
      "all: 8\n",
      "before: 16\n",
      "big: 9\n",
      "black: 10\n",
      "book: 6\n",
      "candy: 13\n",
      "chair: 7\n",
      "clothes: 5\n",
      "computer: 14\n",
      "deaf: 11\n",
      "drink: 15\n",
      "eat: 7\n",
      "fine: 9\n",
      "go: 15\n",
      "help: 14\n",
      "no: 11\n",
      "thin: 16\n",
      "walk: 11\n",
      "who: 14\n",
      "yes: 12\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count videos per word\n",
    "word_counts = Counter([item[\"gloss\"] for item in filtered_videos])\n",
    "\n",
    "# Display counts\n",
    "for word, count in word_counts.items():\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Optional: show as sorted list\n",
    "print(\"\\nSorted by word:\")\n",
    "for word in sorted(word_counts.keys()):\n",
    "    print(f\"{word}: {word_counts[word]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d774438-0608-4162-9203-72da09fed451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original counts per word:\n",
      "book: 6\n",
      "drink: 15\n",
      "computer: 14\n",
      "before: 16\n",
      "chair: 7\n",
      "go: 15\n",
      "clothes: 5\n",
      "who: 14\n",
      "candy: 13\n",
      "deaf: 11\n",
      "fine: 9\n",
      "help: 14\n",
      "no: 11\n",
      "thin: 16\n",
      "walk: 11\n",
      "yes: 12\n",
      "all: 8\n",
      "black: 10\n",
      "eat: 7\n",
      "big: 9\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter([item[\"gloss\"] for item in filtered_videos])\n",
    "print(\"Original counts per word:\")\n",
    "for word, count in word_counts.items():\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb974b34-7bbe-4004-869d-523aa425b06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5 videos per word to balance\n"
     ]
    }
   ],
   "source": [
    "min_count = min(word_counts.values())\n",
    "print(\"Using\", min_count, \"videos per word to balance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2a4628c-ab8b-4c4e-8b61-509e4a62a687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total balanced samples: 100\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "balanced_videos = []\n",
    "for word in word_counts.keys():\n",
    "    # Get all videos for this word\n",
    "    word_videos = [v for v in filtered_videos if v[\"gloss\"] == word]\n",
    "    # Randomly select min_count videos\n",
    "    balanced_videos.extend(random.sample(word_videos, min_count))\n",
    "\n",
    "print(\"Total balanced samples:\", len(balanced_videos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "566a781f-3676-4e95-adb2-9cab3502fc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: all\n",
      "1: before\n",
      "2: big\n",
      "3: black\n",
      "4: book\n",
      "5: candy\n",
      "6: chair\n",
      "7: clothes\n",
      "8: computer\n",
      "9: deaf\n",
      "10: drink\n",
      "11: eat\n",
      "12: fine\n",
      "13: go\n",
      "14: help\n",
      "15: no\n",
      "16: thin\n",
      "17: walk\n",
      "18: who\n",
      "19: yes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labels = [item[\"gloss\"] for item in balanced_videos]\n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "encoded_labels = le.transform(labels)\n",
    "\n",
    "for i, word in enumerate(le.classes_):\n",
    "    print(f\"{i}: {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c63d1ba-8f68-4965-94d9-78302c10d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Hand landmarks extraction\n",
    "def extract_hand_landmarks(video_path, n_frames=32):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    if total_frames == 0:\n",
    "        # If video cannot be read, return all zeros\n",
    "        return np.zeros((n_frames, 42), dtype=np.float32)\n",
    "    \n",
    "    frame_indices = np.linspace(0, max(total_frames-1, 1), n_frames, dtype=int)\n",
    "    \n",
    "    hands_data = []\n",
    "    with mp.solutions.hands.Hands(static_image_mode=False, max_num_hands=1) as hands:\n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or frame is None:\n",
    "                hands_data.append([0]*42)  # failed frame\n",
    "                continue\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(frame_rgb)\n",
    "            if results.multi_hand_landmarks:\n",
    "                landmarks = results.multi_hand_landmarks[0]\n",
    "                frame_landmarks = []\n",
    "                for lm in landmarks.landmark:\n",
    "                    frame_landmarks.append(lm.x)\n",
    "                    frame_landmarks.append(lm.y)\n",
    "                hands_data.append(frame_landmarks)\n",
    "            else:\n",
    "                hands_data.append([0]*42)  # no hand detected\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Final check: pad if still shorter\n",
    "    while len(hands_data) < n_frames:\n",
    "        hands_data.append([0]*42)\n",
    "    \n",
    "    hands_data = hands_data[:n_frames]  # truncate if longer\n",
    "    return np.array(hands_data, dtype=np.float32)\n",
    "\n",
    "# PyTorch Dataset\n",
    "class SignDataset(Dataset):\n",
    "    def __init__(self, videos, labels, n_frames=32):\n",
    "        self.videos = videos\n",
    "        self.labels = labels\n",
    "        self.n_frames = n_frames\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.videos[idx][\"video_path\"]\n",
    "        X = extract_hand_landmarks(video_path, self.n_frames)\n",
    "        y = self.labels[idx]\n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "dataset = SignDataset(balanced_videos, encoded_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0102d281-b208-466f-955e-85288085fec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 80, Validation samples: 20\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42db97b8-aa31-41e9-b8d0-3ae4aa2fccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SignLSTM(nn.Module):\n",
    "    def __init__(self, input_size=42, hidden_size=64, num_layers=2, num_classes=20):\n",
    "        super(SignLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 64).to(x.device)\n",
    "        c0 = torch.zeros(2, x.size(0), 64).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # use last frame output\n",
    "        return out\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SignLSTM(num_classes=len(le.classes_)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc502f14-1eae-41d0-92a4-26d649f335b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignLSTM(nn.Module):\n",
    "    def __init__(self, input_size=42, hidden_size=64, num_layers=2, num_classes=20):\n",
    "        super(SignLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 64).to(x.device)\n",
    "        c0 = torch.zeros(2, x.size(0), 64).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # take last frame\n",
    "        return out\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SignLSTM(num_classes=len(le.classes_)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "356f267b-1787-4b40-bf09-9a7730da5015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3.0102, Val Accuracy: 5.00%\n",
      "Epoch 2/10, Loss: 2.9941, Val Accuracy: 5.00%\n",
      "Epoch 3/10, Loss: 2.9880, Val Accuracy: 0.00%\n",
      "Epoch 4/10, Loss: 2.9744, Val Accuracy: 0.00%\n",
      "Epoch 5/10, Loss: 2.9619, Val Accuracy: 0.00%\n",
      "Epoch 6/10, Loss: 2.9337, Val Accuracy: 0.00%\n",
      "Epoch 7/10, Loss: 2.9195, Val Accuracy: 0.00%\n",
      "Epoch 8/10, Loss: 2.9003, Val Accuracy: 0.00%\n",
      "Epoch 9/10, Loss: 2.8512, Val Accuracy: 0.00%\n",
      "Epoch 10/10, Loss: 2.7640, Val Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            outputs_val = model(X_val)\n",
    "            _, predicted = torch.max(outputs_val, 1)\n",
    "            total += y_val.size(0)\n",
    "            correct += (predicted == y_val).sum().item()\n",
    "    val_acc = 100 * correct / total\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68ae432a-e74e-4a99-9e00-834dcb8e9db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved as 'asl_lstm_model.pth'\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"asl_lstm_model.pth\")\n",
    "print(\"Model weights saved as 'asl_lstm_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6af57c98-94f4-4d01-bd3c-7b168069b8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved as 'asl_lstm_full_model.pth'\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"asl_lstm_full_model.pth\")\n",
    "print(\"Full model saved as 'asl_lstm_full_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f2e87b1-dc82-481b-ad6f-b9a23217d58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignLSTM(\n",
       "  (lstm): LSTM(42, 64, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SignLSTM(num_classes=20)  # same architecture\n",
    "model.load_state_dict(torch.load(\"asl_lstm_model.pth\"))\n",
    "model.eval()  # set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8eeb8-9841-46ea-962c-66cd43aa8f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
